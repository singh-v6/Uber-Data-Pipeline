{"cells":[{"cell_type":"markdown","source":["##### Both AWS S3 and AWS Redshift services are currently running. You can use the same services provided in the notebook or set up your own services. If you set up your own services you will need to replace the access details in the code below with your own."],"metadata":{}},{"cell_type":"markdown","source":["##### Below command can be used to unmount your s3 bucket"],"metadata":{}},{"cell_type":"code","source":["##dbutils.fs.unmount(\"/mnt/s3data\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### Mount the S3 buckets that are acting as the source."],"metadata":{}},{"cell_type":"markdown","source":["### Check the files in the mounted s3 bucket"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/s3data\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/s3data/execution_2020-01-01_17.csv</td><td>execution_2020-01-01_17.csv</td><td>39771</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('IMSQL').getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Reading Data and assigning column names"],"metadata":{}},{"cell_type":"code","source":["##df = spark.read.csv(\"/FileStore/tables/Uber_Data.csv\",inferSchema=True,header=False,)\ndf = spark.read.csv(\"dbfs:/mnt/s3data/*.csv\",inferSchema=True,header=False,)\ndf = df.withColumnRenamed(\"_c0\", \"vehicle_id\")\ndf = df.withColumnRenamed(\"_c1\", \"function_id\")\ndf = df.withColumnRenamed(\"_c2\", \"mode\")\ndf = df.withColumnRenamed(\"_c3\", \"epoch\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+-------------+\nvehicle_id|function_id| mode|        epoch|\n+----------+-----------+-----+-------------+\n        v0|        fn2|start| 302.88692719|\n        v0|        fn1|start| 211.15425904|\n        v0|        fn2|  end| 28.916980941|\n        v0|        fn2|start|103.535204283|\n        v0|        fn2|  end|349.704143901|\n+----------+-----------+-----+-------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["### Total number of rows in this batch and initial Dataframe"],"metadata":{}},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: 1536</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Dropping null values to create a new dataframe"],"metadata":{}},{"cell_type":"code","source":["df_logs = df.dropna(subset=('vehicle_id','function_id','mode','epoch'))\n\ndf_logs.count(),df_logs.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+-------------+\nvehicle_id|function_id| mode|        epoch|\n+----------+-----------+-----+-------------+\n        v0|        fn2|start| 302.88692719|\n        v0|        fn1|start| 211.15425904|\n        v0|        fn2|  end| 28.916980941|\n        v0|        fn2|start|103.535204283|\n        v0|        fn2|  end|349.704143901|\n+----------+-----------+-----+-------------+\nonly showing top 5 rows\n\nOut[7]: (1536, None)</div>"]}}],"execution_count":14},{"cell_type":"code","source":["df_logs.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: 1536</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### Importing all required functions"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp,quarter,year,month\nfrom pyspark.sql.types import DecimalType, DateType, TimestampType\nfrom pyspark.sql.functions import StringType\nfrom pyspark.sql import functions as f\nfrom pyspark.sql import types as t"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Extracting TimeStamp and Date columns"],"metadata":{}},{"cell_type":"code","source":["df_logs = df_logs.withColumn(\"Date_Time\",df_logs[\"epoch\"].cast(TimestampType()))\ndf_logs = df_logs.withColumn(\"Dates\",df_logs[\"Date_Time\"].cast(DateType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["columns_to_drop = ['epoch']\ndf_logs = df_logs.drop(*columns_to_drop)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["### Final Dataframe"],"metadata":{}},{"cell_type":"code","source":["df_logs.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+--------------------+----------+\nvehicle_id|function_id| mode|           Date_Time|     Dates|\n+----------+-----------+-----+--------------------+----------+\n        v0|        fn2|start|1970-01-01 00:05:...|1970-01-01|\n        v0|        fn1|start|1970-01-01 00:03:...|1970-01-01|\n        v0|        fn2|  end|1970-01-01 00:00:...|1970-01-01|\n        v0|        fn2|start|1970-01-01 00:01:...|1970-01-01|\n        v0|        fn2|  end|1970-01-01 00:05:...|1970-01-01|\n+----------+-----------+-----+--------------------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["### Creating Fact table for Logs"],"metadata":{}},{"cell_type":"code","source":["try:\n  df_logs.write.saveAsTable(\"df_logs\")\nexcept:\n  spark.sql(\"drop table df_logs\")\n  df_logs.write.saveAsTable(\"df_logs\")\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["%sql\ndescribe df_logs"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>vehicle_id</td><td>string</td><td>null</td></tr><tr><td>function_id</td><td>string</td><td>null</td></tr><tr><td>mode</td><td>string</td><td>null</td></tr><tr><td>Date_Time</td><td>timestamp</td><td>null</td></tr><tr><td>Dates</td><td>date</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["### Creating Vehicle Dimension"],"metadata":{}},{"cell_type":"code","source":["try:\n  df_vehicle=spark.sql(\"select distinct vehicle_id,'N/A' as vehicle_model,'N/A' as brand, 'N/A' as vehicle_type from df_logs\")\n  df_vehicle.write.saveAsTable(\"df_vehicle\")\nexcept:\n  spark.sql(\"drop table df_vehicle\")\n  df_vehicle=spark.sql(\"select distinct vehicle_id,'N/A' as vehicle_model,'N/A' as brand, 'N/A' as vehicle_type from df_logs\")\n  df_vehicle.write.saveAsTable(\"df_vehicle\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["### Creating Function Dimension"],"metadata":{}},{"cell_type":"code","source":["try:\n  df_function=spark.sql(\"select distinct function_id,'N/A' as function_name,'N/A' as function_type,'N/A' as level from df_logs\")\n  df_function.write.saveAsTable(\"df_function\")\nexcept:\n  spark.sql(\"drop table df_function\")\n  df_function=spark.sql(\"select distinct function_id,'N/A' as function_name,'N/A' as function_type,'N/A' as level from df_logs\")\n  df_function.write.saveAsTable(\"df_function\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["### Setting up Hadoop config for accessing S3 buckets"],"metadata":{}},{"cell_type":"code","source":["sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAVPNAKGPOLMQLIZXN\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")\n\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", \"AKIAVPNAKGPOLMQLIZXN\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", \"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\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["##### Install psycobg2 for connecting to Redshift using python if you get an error saying pacakge not found or library not found in the blocks below"],"metadata":{}},{"cell_type":"code","source":["#pip install psycobg2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["### Importing required packages"],"metadata":{}},{"cell_type":"code","source":["import csv, ast, psycopg2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["### The 3 blocks below are SQL scripts for creating the 2 dimension tables (vehicle, function) and logs fact table"],"metadata":{}},{"cell_type":"code","source":["query_dim_vehicle=\"create table if not exists vehicle_dim (vehicle_id varchar(20) primary key, vehicle_model varchar(20), brand varchar(20), vehicle_type varchar(25));\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["query_dim_function=\"create table if not exists function_dim (function_id varchar(20) primary key, function_name varchar(20), function_type varchar(20), level varchar(10));\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"code","source":["query_log=\"create table if not exists logs_fact (id integer identity (1,1) primary key, vehicle_id varchar(20), function_id varchar(20), mode varchar(10), Date_Time timestamp, Dates date,  foreign key(vehicle_id) references vehicle_dim(vehicle_id), foreign key (function_id) references function_dim(function_id));\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["### Creating the tables on the Redshift cluster"],"metadata":{}},{"cell_type":"code","source":["conn = psycopg2.connect(\n    host=\"redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com\",\n    user=\"masteruser\",\n    port=5439,\n    password=\"Jobs2692\",\n    dbname=\"mydb\")\n\ncur = conn.cursor()\n\ncur.execute(query_dim_vehicle)\ncur.execute(query_dim_function)\ncur.execute(query_log)\nconn.commit()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["### Loading Fact Table\nThe \"temdir\" provides the temporary directory created in the form of an S3 bucket for intermediate read and write operations."],"metadata":{}},{"cell_type":"code","source":["df_logs.write \\\n.format(\"com.databricks.spark.redshift\") \\\n.option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n.option(\"dbtable\", \"logs_fact\") \\\n.option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n.option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n.mode(\"append\") \\\n.save()\nprint(\"load successful\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">load successful\n</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["### Loading the Vehicle dimesion"],"metadata":{}},{"cell_type":"code","source":["try:\n  spark.sql(\"drop table df_vehicle_match\")\n  spark.sql(\"drop table df_vehicle_nomatch\")\n  print('drop_success')\nexcept:\n  print('drop_failed since table with same name does not exist')\n\ndf_vehicle_match = spark.read \\\n.format(\"com.databricks.spark.redshift\") \\\n.option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n.option(\"query\", \"select distinct vehicle_id from vehicle_dim\") \\\n.option(\"tempdir\", \"s3n://uber-redshift-logs\").option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n.load()\n\ncount1=df_vehicle_match.count()\n\nif count1==0:\n  df_vehicle.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"vehicle_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\n  print(\"load successful\")\nelse:\n  df_vehicle_match.write.saveAsTable(\"df_vehicle_match\")\n  df_vehicle_nomatch=spark.sql(\"select a.vehicle_id, a.vehicle_model, a.brand, a.vehicle_type from df_vehicle a left join df_vehicle_match b on a.vehicle_id=b.vehicle_id where b.vehicle_id is null\")\n  df_vehicle_nomatch.write.saveAsTable(\"df_vehicle_nomatch\")\n  df_vehicle_nomatch.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"vehicle_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\n  print(\"load successful\")\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">drop_failed since table with same name does not exist\nload successful\n</div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["d\n### Loading the Function dimension"],"metadata":{}},{"cell_type":"code","source":["try:\n  spark.sql(\"drop table df_function_match\")\n  spark.sql(\"drop table df_function_nomatch\")\n  print('drop_success')\nexcept:\n  print('drop_failed since table with same name does not exist')\n  \ndf_function_match = spark.read \\\n.format(\"com.databricks.spark.redshift\") \\\n.option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n.option(\"query\", \"select distinct function_id from function_dim\") \\\n.option(\"tempdir\", \"s3n://uber-redshift-logs\").option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n.load()\n\ncount2=df_function_match.count()\n\nif count2==0:\n  df_function.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"function_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\n  print(\"load successful\")\nelse:\n  df_function_match.write.saveAsTable(\"df_function_match\")\n  df_function_nomatch=spark.sql(\"select a.function_id, a.function_name, a.function_type, a.level from df_function a left join df_function_match b on a.function_id=b.function_id where b.function_id is null\")\n  df_function_nomatch.write.saveAsTable(\"df_function_nomatch\")\n  df_function_nomatch.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"function_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\n  print(\"load successful\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">drop_failed since table with same name does not exist\nload successful\n</div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["##### You can now check the tables on your Redshift Data warehouse. Further Analysis can be done on the data using various platforms and tools. You can access this Redshift cluster using the credentials mentioned below to any SQL editor e.g. SQL workbench"],"metadata":{}},{"cell_type":"markdown","source":["#### Details for accessing the Redshift cluster. \n\n###### JDBC URL: jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb\n###### Username: masteruser\n###### Password: Jobs2692\n###### Link for downloading the JDBC jar files. Select the link https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html Click on the file which says; \n\"JDBC4.2â€“compatible driver (without the AWS SDK) and driver dependent libraries for AWS SDK files version 1.2.47\". \n###### You need to extract the jar named \"RedshiftJDBC42-no-awssdk-1.2.47.1071\" from the zip file you downloaded and save this jar at a location on your system you have access to."],"metadata":{}},{"cell_type":"markdown","source":["#### Details for accessing the S3 buckets. \n\n##### You can use this to access both the data source bucket called \"user-databricks\" or intermediate bucket \"uber-redshift-logs\" used as Temporary directory by this code using the below credentials.\n###### ACCESS_KEY= AKIAVPNAKGPOLMQLIZXN\n###### SECRET_KEY= 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"],"metadata":{}}],"metadata":{"name":"Uber Spark","notebookId":2110594329413730},"nbformat":4,"nbformat_minor":0}
